/**
 * Blog article: where-ai-fails-real-world-software-development
 * Where AI Still Fails in Real-World Software Development.
 */

export default {
  slug: "where-ai-fails-real-world-software-development",
  title: "Where AI Still Fails in Real-World Software Development",
  excerpt: "Honest look at where AI coding tools still fail: architecture, edge cases, security, consistency, and domain nuance. With internal links to trade-offs, code quality, IDEs, and testing.",
  date: "2026-01-21",
  topic: "Architecture",
  keywords: ["Where AI Still Fails in Real-World Software Development", "Where Ai Fails Real World Software Development", "Where Ai Fails Real World Software Development best practices", "how to where ai fails real world software development", "where ai fails real world software development in .NET", "where ai fails real world software development guide", "where ai fails real world software development for enterprise", "where ai fails real world software development patterns", "when to use where ai fails real world software development", "where ai fails real world software development tutorial", "where ai fails real world software development examples", "where ai fails real world software development in C#", "where ai fails real world software development overview", "where ai fails real world software development implementation", "understanding where ai fails real world software development", "where ai fails real world software development for developers", "where ai fails real world software development checklist", "where ai fails real world software development tips", "where ai fails real world software development deep dive", "where ai fails real world software development comparison", "where ai fails real world software development vs alternatives", "where ai fails real world software development .NET Core", "where ai fails real world software development Azure", "where ai fails real world software development explained", "where ai fails real world software development when to use", "where ai fails real world software development enterprise", "where ai fails real world software development .NET", "what is where ai fails real world software development", "where ai fails real world software development summary", "where ai fails real world software development introduction", "where ai fails real world software development fundamentals", "where ai fails real world software development step by step", "where ai fails real world software development complete guide", "where ai fails real world software development for beginners", "where ai fails real world software development advanced", "where ai fails real world software development production", "where ai fails real world software development real world", "where ai fails real world software development example code", "where ai fails real world software development C# example", "where ai fails real world software development .NET example", "learn where ai fails real world software development", "where ai fails real world software development learn", "where ai fails real world software development reference", "where ai fails real world software development cheat sheet", "where ai fails real world software development pitfalls", "where ai fails real world software development common mistakes", "where ai fails real world software development performance", "where ai fails real world software development optimization", "where ai fails real world software development security", "where ai fails real world software development testing", "where ai fails real world software development unit test", "where ai fails real world software development integration", "where ai fails real world software development migration", "where ai fails real world software development from scratch", "where ai fails real world software development 2024", "where ai fails real world software development 2025", "best where ai fails real world software development", "where ai fails real world software development best", "pro where ai fails real world software development", "where ai fails real world software development expert", "where ai fails real world software development consultant", "where ai fails real world software development services", "where ai fails real world software development course", "where ai fails real world software development workshop", "where ai fails real world software development webinar", "where ai fails real world software development blog", "where ai fails real world software development article", "where ai fails real world software development post", "why where ai fails real world software development", "when where ai fails real world software development", "where where ai fails real world software development", "where ai fails real world software development in .NET 6", "where ai fails real world software development in .NET 7", "where ai fails real world software development in .NET 8", "where ai fails real world software development for C#", "where ai fails real world software development for Angular", "where ai fails real world software development for Vue", "where ai fails real world software development for React", "where ai fails real world software development for Azure", "where ai fails real world software development for microservices", "where ai fails real world software development for API", "where ai fails real world software development for database", "where ai fails real world software development for testing", "where ai fails real world software development for DevOps", "where ai fails real world software development for senior developers", "where ai fails real world software development for team", "where ai fails real world software development for production", "where ai fails real world software development for scale", "where ai fails real world software development for refactoring", "where ai fails real world software development for enterprise applications", "where ai fails real world software development for startup", "where ai fails real world software development in 2024", "where ai fails real world software development in 2025", "where ai fails real world software development in 2026", "where ai fails real world software development code sample", "where ai fails real world software development code example", "where ai fails real world software development sample code", "where ai fails real world software development full example", "where ai fails real world software development working example", "where ai fails real world software development practical where ai fails real world software development", "where ai fails real world software development real world example", "where ai fails real world software development use case", "where ai fails real world software development use cases", "where ai fails real world software development scenario", "where ai fails real world software development scenarios", "where ai fails real world software development pattern", "where ai fails real world software development approach", "where ai fails real world software development approaches", "where ai fails real world software development strategy", "where ai fails real world software development strategies", "where ai fails real world software development technique", "where ai fails real world software development techniques", "where ai fails real world software development method", "where ai fails real world software development methods", "where ai fails real world software development solution", "where ai fails real world software development solutions", "where ai fails real world software development implementation guide", "where ai fails real world software development getting started", "where ai fails real world software development quick start", "where ai fails real world software development overview guide", "where ai fails real world software development comprehensive guide", "where ai fails real world software development detailed guide", "where ai fails real world software development practical guide", "where ai fails real world software development developer guide", "where ai fails real world software development engineer guide", "where ai fails real world software development architect guide", "where ai fails real world software development for architects", "where ai fails real world software development for backend", "where ai fails real world software development for tech leads", "where ai fails real world software development for senior devs", "benefits of where ai fails real world software development", "advantages of where ai fails real world software development", "alternatives to where ai fails real world software development", "compared to where ai fails real world software development", "intro to where ai fails real world software development", "basics of where ai fails real world software development", "where ai fails real world software development tips and tricks", "where ai fails real world software development production-ready", "where ai fails real world software development enterprise-grade", "where ai fails real world software development with Docker", "where ai fails real world software development with Kubernetes", "where ai fails real world software development in ASP.NET Core", "where ai fails real world software development with Entity Framework", "where ai fails real world software development with EF Core", "where ai fails real world software development modern", "where ai fails real world software development updated", "where ai fails real world software development latest", "where ai fails real world software development walkthrough", "where ai fails real world software development hands-on", "where ai fails real world software development practical examples", "where ai fails real world software development real-world examples", "where ai fails real world software development common pitfalls", "where ai fails real world software development gotchas", "where ai fails real world software development FAQ", "where ai fails real world software development FAQs", "where ai fails real world software development Q&A", "where ai fails real world software development interview questions", "where ai fails real world software development interview", "where ai fails real world software development certification", "where ai fails real world software development training", "where ai fails real world software development video", "where ai fails real world software development series", "where ai fails real world software development part 1", "where ai fails real world software development core concepts", "where ai fails real world software development key concepts", "where ai fails real world software development recap", "where ai fails real world software development takeaways", "where ai fails real world software development conclusion", "where ai fails real world software development next steps", "where ai fails real world software development further reading", "where ai fails real world software development resources", "where ai fails real world software development tools", "where ai fails real world software development libraries", "where ai fails real world software development frameworks", "where ai fails real world software development NuGet", "where ai fails real world software development package", "where ai fails real world software development GitHub", "where ai fails real world software development open source", "where ai fails real world software development community", "where ai fails real world software development Microsoft docs", "where ai fails real world software development documentation", "where ai fails real world software development official guide", "where ai fails real world software development official tutorial", "Where", "Where guide", "Where tutorial", "Where best practices", "Where in .NET", "Where in C#", "Where for developers", "Where examples", "Where patterns", "Where overview", "Where introduction", "Where deep dive", "Where explained", "Where how to", "Where what is", "Where when to use", "Where for enterprise", "Where .NET Core", "Where Azure", "Where C#", "Where with .NET", "Where with C#", "Where with Azure", "Where with Angular", "Where with Vue", "Where with React", "Where with Entity Framework", "Where with SQL Server", "Where step by step", "Where complete guide", "Where from scratch", "Where 2024", "Where 2025", "Where 2026", "Where code example", "Where sample code", "Where implementation", "Where real world", "Where production", "Where for beginners", "Where advanced", "Where for architects", "Where for backend", "Where for API", "Where in ASP.NET Core", "Where with EF Core", "Where tutorial 2024", "Where guide 2025", "Where best practices 2024", "Where C# examples", "Where .NET examples", "Where implementation guide", "Where how to implement", "Where benefits", "Where advantages", "Where pitfalls", "Where alternatives", "Where compared", "Where intro", "Where basics", "Where tips and tricks", "Where production-ready", "Where enterprise-grade", "Where maintainable", "Where testable", "Where refactoring", "Where modern", "Where updated", "Where latest", "Where for tech leads", "Where for senior devs", "Where with Docker", "Where with Kubernetes", "Where in .NET 8", "Where in .NET 7", "Where in .NET 6", "Where Ai", "Where Ai guide", "Where Ai tutorial", "Where Ai best practices", "Where Ai in .NET", "Where Ai in C#", "Where Ai for developers", "Where Ai examples", "Where Ai patterns", "Where Ai overview", "Where Ai introduction", "Where Ai deep dive", "Where Ai explained", "Where Ai how to", "Where Ai what is", "Where Ai when to use", "Where Ai for enterprise", "Where Ai .NET Core", "Where Ai Azure", "Where Ai C#", "Where Ai with .NET", "Where Ai with C#", "Where Ai with Azure", "Where Ai with Angular", "Where Ai with Vue", "Where Ai with React", "Where Ai with Entity Framework", "Where Ai with SQL Server", "Where Ai step by step", "Where Ai complete guide", "Where Ai from scratch", "Where Ai 2024", "Where Ai 2025", "Where Ai 2026", "Where Ai code example", "Where Ai sample code", "Where Ai implementation", "Where Ai real world", "Where Ai production", "Where Ai for beginners", "Where Ai advanced", "Where Ai for architects", "Where Ai for backend", "Where Ai for API", "Where Ai in ASP.NET Core", "Where Ai with EF Core", "Where Ai tutorial 2024", "Where Ai guide 2025", "Where Ai best practices 2024", "Where Ai C# examples", "Where Ai .NET examples", "Where Ai implementation guide", "Where Ai how to implement", "Where Ai benefits", "Where Ai advantages", "Where Ai pitfalls", "Where Ai alternatives", "Where Ai compared", "Where Ai intro", "Where Ai basics", "Where Ai tips and tricks", "Where Ai production-ready", "Where Ai enterprise-grade", "Where Ai maintainable", "Where Ai testable", "Where Ai refactoring", "Where Ai modern", "Where Ai updated", "Where Ai latest", "Where Ai for tech leads", "Where Ai for senior devs", "Where Ai with Docker", "Where Ai with Kubernetes", "Where Ai in .NET 8", "Where Ai in .NET 7", "Where Ai in .NET 6", "Where Ai Fails", "Where Ai Fails guide", "Where Ai Fails tutorial", "Where Ai Fails best practices", "Where Ai Fails in .NET", "Where Ai Fails in C#", "Where Ai Fails for developers", "Where Ai Fails examples", "Where Ai Fails patterns", "Where Ai Fails overview", "Where Ai Fails introduction", "Where Ai Fails deep dive", "Where Ai Fails explained", "Where Ai Fails how to", "Where Ai Fails what is", "Where Ai Fails when to use", "Where Ai Fails for enterprise", "Where Ai Fails .NET Core", "Where Ai Fails Azure", "Where Ai Fails C#", "Where Ai Fails with .NET", "Where Ai Fails with C#", "Where Ai Fails with Azure", "Where Ai Fails with Angular", "Where Ai Fails with Vue", "Where Ai Fails with React", "Where Ai Fails with Entity Framework", "Where Ai Fails with SQL Server", "Where Ai Fails step by step", "Where Ai Fails complete guide", "Where Ai Fails from scratch", "Where Ai Fails 2024", "Where Ai Fails 2025", "Where Ai Fails 2026", "Where Ai Fails code example", "Where Ai Fails sample code", "Where Ai Fails implementation", "Where Ai Fails real world", "Where Ai Fails production", "Where Ai Fails for beginners", "Where Ai Fails advanced", "Where Ai Fails for architects", "Where Ai Fails for backend", "Where Ai Fails for API", "Where Ai Fails in ASP.NET Core", "Where Ai Fails with EF Core", "Where Ai Fails tutorial 2024", "Where Ai Fails guide 2025", "Where Ai Fails best practices 2024", "Where Ai Fails C# examples", "Where Ai Fails .NET examples", "Where Ai Fails implementation guide", "Where Ai Fails how to implement", "Where Ai Fails benefits", "Where Ai Fails advantages", "Where Ai Fails pitfalls", "Where Ai Fails alternatives", "Where Ai Fails compared", "Where Ai Fails intro", "Where Ai Fails basics", "Where Ai Fails tips and tricks", "Where Ai Fails production-ready", "Where Ai Fails enterprise-grade", "Where Ai Fails maintainable", "Where Ai Fails testable", "Where Ai Fails refactoring", "Where Ai Fails modern", "Where Ai Fails updated", "Where Ai Fails latest", "Where Ai Fails for tech leads", "Where Ai Fails for senior devs", "Where Ai Fails with Docker", "Where Ai Fails with Kubernetes", "Where Ai Fails in .NET 8", "Where Ai Fails in .NET 7", "Where Ai Fails in .NET 6", "Where Ai Fails Real", "Where Ai Fails Real guide", "Where Ai Fails Real tutorial", "Where Ai Fails Real best practices", "Where Ai Fails Real in .NET", "Where Ai Fails Real in C#", "Where Ai Fails Real for developers", "Where Ai Fails Real examples", "Where Ai Fails Real patterns", "Where Ai Fails Real overview", "Where Ai Fails Real introduction", "Where Ai Fails Real deep dive", "Where Ai Fails Real explained", "Where Ai Fails Real how to", "Where Ai Fails Real what is", "Where Ai Fails Real when to use", "Where Ai Fails Real for enterprise", "Where Ai Fails Real .NET Core", "Where Ai Fails Real Azure", "Where Ai Fails Real C#", "Where Ai Fails Real with .NET", "Where Ai Fails Real with C#", "Where Ai Fails Real with Azure", "Where Ai Fails Real with Angular", "Where Ai Fails Real with Vue", "Where Ai Fails Real with React", "Where Ai Fails Real with Entity Framework", "Where Ai Fails Real with SQL Server", "Where Ai Fails Real step by step", "Where Ai Fails Real complete guide", "Where Ai Fails Real from scratch", "Where Ai Fails Real 2024", "Where Ai Fails Real 2025", "Where Ai Fails Real 2026", "Where Ai Fails Real code example", "Where Ai Fails Real sample code", "Where Ai Fails Real implementation", "Where Ai Fails Real real world", "Where Ai Fails Real production", "Where Ai Fails Real for beginners", "Where Ai Fails Real advanced", "Where Ai Fails Real for architects", "Where Ai Fails Real for backend", "Where Ai Fails Real for API", "Where Ai Fails Real in ASP.NET Core", "Where Ai Fails Real with EF Core", "Where Ai Fails Real tutorial 2024", "Where Ai Fails Real guide 2025", "Where Ai Fails Real best practices 2024", "Where Ai Fails Real C# examples", "Where Ai Fails Real .NET examples", "Where Ai Fails Real implementation guide", "Where Ai Fails Real how to implement", "Where Ai Fails Real benefits", "Where Ai Fails Real advantages", "Where Ai Fails Real pitfalls", "Where Ai Fails Real alternatives"],
  relatedServices: ["full-stack-development", "technical-leadership"],
  relatedProjects: [],
  relatedArticleSlugs: ["trade-offs-ai-code-generation", "impact-ai-tools-code-quality-maintainability", "ai-ides-what-they-get-right-wrong", "current-state-ai-coding-tools-2026"],
  author: "Waqas Ahmad",
  content: `## Introduction

**AI coding tools** have improved sharply, but they **still fail** in real-world software development in **predictable** ways: **architecture** decisions, **rare edge cases**, **security-sensitive** code, **consistency** across a large codebase, and **domain** nuance. This article is an **honest** map of **where AI still fails** so you can use it **where it helps** and **verify** where it does not.

Knowing **where** failure is likely lets you **target** AI at **safe** tasks (boilerplate, explanations, scaffolding) and **apply** **review** and **tests** where it is **not**—architecture, security, edge cases, and business rules. We cover **architecture and design**, **edge cases and correctness**, **security**, **consistency and style**, **domain and business rules**, and **what to do** (review, tests, ownership). For the bigger picture see [The Current State of AI Coding Tools in 2026](/blog/current-state-ai-coding-tools-2026); for trade-offs see [The Trade-Offs of Relying on AI for Code Generation](/blog/trade-offs-ai-code-generation); for quality see [Impact of AI Tools on Code Quality and Maintainability](/blog/impact-ai-tools-code-quality-maintainability).

**Who this is for:** Developers and leads who want a **clear** list of **failure modes** and **concrete** mitigations so they can **use** AI without **trusting** it in the wrong places.

If you are new, start with [Topics covered](#topics-covered) and [Where AI fails at a glance](#where-ai-fails-at-a-glance).

## Topics covered

- [Decision Context](#decision-context)
- [Why “where AI fails” matters](#why-where-ai-fails-matters)
- [Where AI fails at a glance](#where-ai-fails-at-a-glance)
- [Architecture and design](#architecture-and-design)
- [Edge cases and correctness](#edge-cases-and-correctness)
- [Security](#security)
- [Consistency and style](#consistency-and-style)
- [Domain and business rules](#domain-and-business-rules)
- [What to do: review, tests, ownership](#what-to-do-review-tests-ownership)
- [Real-world failure examples](#real-world-failure-examples)
- [Code-level examples: what failure looks like in real code](#code-level-examples-what-failure-looks-like-in-real-code)
- [Architecture failures in depth](#architecture-failures-in-depth)
- [Edge-case and correctness failures in depth](#edge-case-and-correctness-failures-in-depth)
- [Consistency and style failures in depth](#consistency-and-style-failures-in-depth)
- [Mitigation checklist](#mitigation-checklist-before-during-and-after)
- [Failure mode summary table](#failure-mode-summary-table)
- [By language and stack](#by-language-and-stack)
- [When to involve security or domain experts](#when-to-involve-security-or-domain-experts)
- [Key terms](#key-terms)
- [Related reading](#related-reading)
- [Common issues and challenges](#common-issues-and-challenges)
- [Best practices and pitfalls](#best-practices-and-pitfalls)
- [Quick reference: when to verify](#quick-reference-when-to-verify)
- [Summary](#summary)
- [Position & Rationale](#position--rationale)
- [Trade-Offs & Failure Modes](#trade-offs--failure-modes)
- [What Most Guides Miss](#what-most-guides-miss)
- [Decision Framework](#decision-framework)
- [Key Takeaways](#key-takeaways)
- [When I Would Use This Again — and When I Wouldn't](#when-i-would-use-this-again--and-when-i-wouldnt)
- [Frequently Asked Questions](#frequently-asked-questions)

---

## Decision Context

- **When this applies:** Teams or developers using AI coding tools who want a clear picture of where current tools tend to fail so they can set boundaries and review accordingly.
- **When it doesn't:** Teams that don't use AI or that only want tool recommendations. This article is about failure modes, not tools.
- **Scale:** Any team size; the failure modes (architecture, edge cases, security, consistency, domain) are structural.
- **Constraints:** Mitigation requires review, tests, and explicit boundaries; without them, reliance on AI in weak areas is riskier.
- **Non-goals:** This article doesn't argue that AI is useless; it states where it tends to fail and how to mitigate.


---

## Why “where AI fails” matters

Knowing **where AI fails** lets you **target** it at tasks it handles well (boilerplate, patterns, explanations) and **avoid** trusting it for **high-stakes** areas (architecture, security, rare bugs) without **verification**. That reduces **debt** and **risk**—see [Trade-Offs of Relying on AI for Code Generation](/blog/trade-offs-ai-code-generation) and [What AI IDEs Get Right — and What They Get Wrong](/blog/ai-ides-what-they-get-right-wrong). Without this map, teams often **over-trust** AI in **architecture** or **security** and **under-review** in **edge cases**—leading to **rework** and **incidents** that could have been **caught** by **review** and **tests**.

---

## Where AI fails at a glance

| Area | How AI often fails | What to do |
|------|--------------------|------------|
| **Architecture** | Suggests local optima, ignores constraints | Humans decide; use [Clean Architecture](/blog/clean-architecture-dotnet), [SOLID](/blog/solid-principles-in-practice) |
| **Edge cases** | Misses rare inputs, off-by-one, nulls | Tests, review; [testing strategies](/blog/testing-strategies-unit-integration-e2e) |
| **Security** | Insecure defaults, wrong threat model | Security review; never trust AI alone |
| **Consistency** | Style and patterns drift across files | Linters, style guide, human review |
| **Domain** | Misunderstands business rules | Domain experts and tests |

\`\`\`mermaid
flowchart LR
  subgraph Fails
    A[Architecture]
    E[Edge cases]
    S[Security]
    C[Consistency]
    D[Domain]
  end
  A --> R[Review + Tests]
  E --> R
  S --> R
  C --> R
  D --> R
  style R fill:#059669,color:#fff
\`\`\`

**How to use the table:** For each **area** (architecture, edge cases, security, consistency, domain), the table summarises **how** AI often fails and **what to do**. **Review**, **tests**, and **ownership** are the **common** mitigations; **linters** and **style guides** help **consistency**; **domain** experts and **tests** help **business rules**. Use this as a **checklist** when **reviewing** AI-generated code.

---

## Architecture and design

AI tends to **optimise locally**: it suggests code that **works** in the narrow context you gave, not necessarily **fit** for your [Clean Architecture](/blog/clean-architecture-dotnet), [microservices](/blog/azure-microservices-best-practices), or **scale**. It can **break** dependency rules (e.g. use case importing infrastructure), **ignore** existing patterns (e.g. a new service that does not follow your [repository](/blog/repository-pattern-unit-of-work-dotnet) or [dependency injection](/blog/dependency-injection-dotnet-core) style), and **over-engineer** (e.g. unnecessary abstraction) or **under-engineer** (e.g. logic in the wrong layer). **Ownership:** Humans own **architecture**; use AI for **implementation within** agreed boundaries. See [What AI IDEs Get Right — and What They Get Wrong](/blog/ai-ides-what-they-get-right-wrong). **Concrete failure:** "Add a new API for orders" may produce a **controller** that **calls** the **database** directly instead of going through a **service** and **repository**—**review** must **enforce** layering.

---

## Edge cases and correctness

AI is trained on **common** patterns; **rare** inputs, **boundary** conditions, **nulls**, and **concurrency** are where it **often** fails. Generated code can **look** right and **fail** in production. **What to do:** [Unit and integration tests](/blog/testing-strategies-unit-integration-e2e), **review** with edge cases in mind, and **never** assume “AI wrote it” means “it’s correct.” See [How AI Is Changing Code Review and Testing](/blog/ai-changing-code-review-testing).

---

## Security

AI can suggest **insecure** defaults (e.g. weak crypto, SQL concatenation, hardcoded secrets), **wrong** threat models, or **outdated** advice. **Never** rely on AI alone for **auth**, **secrets**, **injection**, or **compliance**. **What to do:** Security **review**, [OWASP](/blog/owasp-api-security-top-10) and [Securing APIs](/blog/securing-apis-dotnet), and **treat AI output as untrusted** in security-sensitive paths.

---

## Consistency and style

Across **many files**, AI can **drift** in **naming**, **structure**, and **patterns**. One file may look like your style; the next may not. **What to do:** **Linters**, **formatters**, **style guides**, and **human review** to keep consistency. [Technical leadership](/blog/technical-leadership-remote-teams) can set **norms** and **templates**.

---

## Domain and business rules

AI does **not** know your **business rules**, **regulations**, or **domain** nuance. It can **guess** and get **wrong** (e.g. rounding, dates, eligibility). **What to do:** **Domain experts** and **tests** that encode rules; use AI for **scaffolding**, not **authoritative** business logic. See [Domain-Driven Design](/blog/domain-driven-design-basics) for clarity on boundaries.

---

## What to do: review, tests, ownership

**Review** all AI-generated code—especially for **architecture**, **security**, and **edge cases**. **Tests** (unit, integration) catch **correctness** and **regressions**; [testing strategies](/blog/testing-strategies-unit-integration-e2e) still apply. **Ownership:** Assign **owners** for **design** and **critical** paths; use AI as **assistant**, not **decision-maker**. See [Impact of AI Tools on Code Quality and Maintainability](/blog/impact-ai-tools-code-quality-maintainability) and [What Developers Actually Want From AI Assistants](/blog/what-developers-want-from-ai-assistants). **Norms** ([technical leadership](/blog/technical-leadership-remote-teams)): e.g. "AI output is draft; human review is required for merge."

---

## Real-world failure examples

**Architecture:** A team asked AI to "add a new feature" and received a **monolithic** handler that mixed **HTTP**, **business logic**, and **DB** access. **Fix:** Define **layers** and **prompt** within them (e.g. "add a use case for X in the application layer"); **review** for **dependency** direction. **Edge cases:** Generated code **passed** unit tests for **happy path** but **failed** when **input** was **null** or **empty**. **Fix:** **Add** **edge-case** tests; **review** with **null** and **boundaries** in mind. **Security:** AI suggested **string** concatenation for **SQL** and **hardcoded** config. **Fix:** **Never** trust AI for **auth**, **secrets**, or **injection**; use [OWASP](/blog/owasp-api-security-top-10) and **security** review. **Consistency:** Multi-file generation produced **different** **naming** and **error-handling** style. **Fix:** **Linters**, **formatters**, **human** review. **Domain:** AI **guessed** a **business rule** (e.g. rounding) and got it **wrong**. **Fix:** **Domain** experts and **tests** that **encode** rules; use AI for **scaffolding** only.

---

## Code-level examples: what failure looks like in real code

Below are **exact prompts**, **full** **bad** AI output (what the model returns in theory), **what goes wrong** at build or runtime, and **full** **correct** code so you see **concrete** issues at **code** level—not just description. Use them when **reviewing** or **training** the team.

### Example 1: Architecture — prompt vs full bad vs full good

**Exact prompt:** "Add an endpoint to fetch a user by ID and return their profile."

**What you get in theory (bad AI output):** Controller injects **DbContext**, does **mapping** and **logic** in the controller, and **violates** [Clean Architecture](/blog/clean-architecture-dotnet).

\`\`\`csharp
// BAD: Controller with infra dependency and logic in wrong place
using Microsoft.AspNetCore.Mvc;
using Microsoft.EntityFrameworkCore;

namespace MyApp.Api.Controllers
{
    public class UserController : ControllerBase
    {
        private readonly AppDbContext _db;

        public UserController(AppDbContext db) => _db = db;

        [HttpGet("{id}")]
        public async Task<IActionResult> Get(int id)
        {
            var user = await _db.Users.FindAsync(id);
            if (user == null) return NotFound();
            var profile = new UserProfileDto
            {
                FullName = user.FirstName + " " + user.LastName,
                Email = user.Email,
                CreatedAt = user.CreatedAt
            };
            return Ok(profile);
        }
    }
}
\`\`\`

**What goes wrong at code level:** Controller depends on **infrastructure** (DbContext); **mapping** and **concatenation** logic live in the **API** layer; **hard** to unit-test and to change persistence or profile shape. **Result in theory:** **Rework** when you introduce a use case layer; **tests** require a real or mocked DbContext at the controller.

**Correct approach (full good code):** Controller only delegates; use case and repository live in their layers.

\`\`\`csharp
// GOOD: Controller delegates; no DbContext; use case uses IUserRepository
namespace MyApp.Api.Controllers
{
    public class UserController : ControllerBase
    {
        private readonly IGetUserProfileUseCase _getUserProfileUseCase;

        public UserController(IGetUserProfileUseCase getUserProfileUseCase) =>
            _getUserProfileUseCase = getUserProfileUseCase;

        [HttpGet("{id}")]
        public async Task<IActionResult> Get(int id)
        {
            var result = await _getUserProfileUseCase.Execute(id);
            return result.Match<IActionResult>(Ok, _ => NotFound());
        }
    }
}

// Application layer: IGetUserProfileUseCase.Execute(id) uses IUserRepository, maps to UserProfileDto
// Infrastructure: IUserRepository implemented with DbContext in a separate project
\`\`\`

---

### Example 2: Security — SQL injection and hardcoded secret

**Exact prompt:** "Write code to search users by email and log to the database."

**What you get in theory (bad AI output):** User input **concatenated** into SQL; **hardcoded** connection or secret.

\`\`\`csharp
// BAD: Injection risk + hardcoded secret
public class UserSearchService
{
    private const string ConnectionString = "Server=prod;User=sa;Password=secret123";

    public async Task<User?> FindByEmail(string email)
    {
        var sql = "SELECT * FROM Users WHERE Email = '" + email + "'";
        await using var conn = new SqlConnection(ConnectionString);
        await conn.OpenAsync();
        await using var cmd = new SqlCommand(sql, conn);
        await using var reader = await cmd.ExecuteReaderAsync();
        return reader.Read() ? MapUser(reader) : null;
    }
}
\`\`\`

**What goes wrong at code level:** Input \`email = "'; DROP TABLE Users; --"\` executes arbitrary SQL; **secret** in source control and **compliance** breach. **Result in theory:** **Security** incident and **data loss**.

**Correct approach (full good code):** Parameterised query; configuration for connection.

\`\`\`csharp
// GOOD: Parameterised; no secrets in code
public class UserSearchService
{
    private readonly IConfiguration _config;

    public UserSearchService(IConfiguration config) => _config = config;

    public async Task<User?> FindByEmail(string email)
    {
        var connStr = _config.GetConnectionString("Default");
        await using var conn = new SqlConnection(connStr);
        await conn.OpenAsync();
        await using var cmd = new SqlCommand("SELECT * FROM Users WHERE Email = @Email", conn);
        cmd.Parameters.AddWithValue("@Email", email);
        await using var reader = await cmd.ExecuteReaderAsync();
        return reader.Read() ? MapUser(reader) : null;
    }
}
\`\`\`

---

### Example 3: Edge cases — null and boundaries

**Exact prompt:** "Implement a method that returns the first line item amount for an order."

**What you get in theory (bad AI output):** No null or empty check; **IndexOutOfRangeException** or **NullReferenceException** in production.

\`\`\`csharp
// BAD: Crashes when order or LineItems null/empty
public decimal GetFirstAmount(Order order)
{
    return order.LineItems[0].Amount;
}
\`\`\`

**What goes wrong at code level:** \`order\` null → **NullReferenceException**; \`LineItems\` null or empty → **NullReferenceException** or **IndexOutOfRangeException**. **Result in theory:** **Runtime** crash when orders are empty or partially loaded.

**Correct approach (full good code):** Explicit null and empty handling; documented behaviour.

\`\`\`csharp
// GOOD: Edge cases handled; behaviour clear
public decimal? GetFirstAmount(Order? order)
{
    if (order?.LineItems == null || order.LineItems.Count == 0)
        return null;
    return order.LineItems[0].Amount;
}
\`\`\`

---

### Example 4: Consistency — naming and style drift

**Exact prompt (file A):** "Add a method to get order by ID." **Prompt (file B):** "Add method to fetch order."

**What you get in theory (bad AI output):** Different names and signatures across files; **inconsistent** with existing repo.

\`\`\`csharp
// File A (OrderService.cs): GetOrderByIdAsync
public async Task<Order> GetOrderByIdAsync(int id)
{
    return await _repo.FindById(id);
}

// File B (PaymentService.cs): FetchOrder, sync, different return
public Order FetchOrder(int id)
{
    return _orderRepo.GetById(id);  // blocking; different naming
}
\`\`\`

**What goes wrong at code level:** Two **conventions** in one codebase; **callers** mix \`GetOrderByIdAsync\` and \`FetchOrder\`; **async** vs sync **deadlocks** or confusion. **Result in theory:** **Review** churn and **maintainability** debt.

**Correct approach (full good code):** Single convention; align with style guide and existing repos.

\`\`\`csharp
// GOOD: Single convention (async suffix, Get* naming, Task<T> return)
// OrderService.cs
public async Task<Order?> GetOrderByIdAsync(int id, CancellationToken ct = default) =>
    await _repo.GetByIdAsync(id, ct);

// PaymentService.cs — same pattern
public async Task<Order?> GetOrderByIdAsync(int id, CancellationToken ct = default) =>
    await _orderRepo.GetByIdAsync(id, ct);
\`\`\`

---

### Example 5: Domain — wrong business rule and rounding

**Exact prompt:** "Calculate discount for an order: 10% if total > 100."

**What you get in theory (bad AI output):** Boundary **unclear** (is 100 included?); **rounding** not specified; **no** tests encoding the rule.

\`\`\`csharp
// BAD: Boundary unclear; rounding not specified; no tests
public decimal GetDiscount(Order order)
{
    return order.Total >= 100 ? order.Total * 0.10m : 0;
}
\`\`\`

**What goes wrong at code level:** Product says "discount **above** 100" but code uses \`>=\` (100 gets discount); **currency** rounding can differ (banker's vs half-up); **no** tests so **regressions** go unnoticed. **Result in theory:** **Wrong** discount in production and **disputes**.

**Correct approach (full good code):** Explicit rule; rounding specified; tests encode behaviour.

\`\`\`csharp
// GOOD: Rule explicit; rounding specified; tests encode rule
public decimal GetDiscount(Order order)
{
    if (order.Total <= 100m) return 0m;
    var discount = order.Total * 0.10m;
    return Math.Round(discount, 2, MidpointRounding.AwayFromZero);
}

// Unit tests: GetDiscount_WhenTotal100_Returns0; GetDiscount_WhenTotal100_01_Returns10_00; etc.
\`\`\`

---

**Takeaway:** Use these **exact prompts** and **full** bad/good pairs as **review** checklists. **Architecture** = layers and dependencies; **security** = parameterised queries, no secrets in code; **edge cases** = null, empty, boundaries; **consistency** = naming and async convention; **domain** = rules and rounding **encoded** in tests. See [What to do: review, tests, ownership](#what-to-do-review-tests-ownership) and [Mitigation checklist](#mitigation-checklist-before-during-and-after).

---

## Architecture failures in depth

**Local optima.** AI **suggests** code that **works** in the **narrow** context (e.g. "add an endpoint") but **violates** [Clean Architecture](/blog/clean-architecture-dotnet) (e.g. **controller** calling **repository** directly, or **use case** importing **infrastructure**). **Why:** Models are trained on **snippets** and **common** patterns, not **your** **architecture** doc or **dependency** rules. **Mitigation:** **Humans** own **architecture**; **document** **layers** and **boundaries**; **prompt** within **bounds** ("add a use case for X"); **review** for **dependency** direction and **layer** placement. See [What AI IDEs Get Right and Wrong](/blog/ai-ides-what-they-get-right-wrong).

**Over- and under-engineering.** AI may **add** **unnecessary** abstraction (e.g. **factory** for **one** implementation) or **put** **logic** in the **wrong** place (e.g. **SQL** in a **controller**). **Mitigation:** **Review** for **fit** with **actual** **scale** and **team** **conventions**; use [SOLID](/blog/solid-principles-in-practice) and [design patterns](/blog/design-patterns-overview-creational-structural-behavioral) as **guidance**, not **blind** **acceptance** of AI suggestions.

---

## Edge-case and correctness failures in depth

**Rare inputs and boundaries.** **Null**, **empty** list, **zero**, **negative** values, **max** length—AI **often** **generates** **happy-path** code and **misses** these. **Off-by-one** and **boundary** **bugs** are **common** in **generated** **loops** and **conditionals**. **Mitigation:** [Unit and integration tests](/blog/testing-strategies-unit-integration-e2e) that **cover** **edge** cases; **review** with **null** and **boundaries** in mind; **never** assume "AI wrote it" means "it's correct." See [How AI Is Changing Code Review and Testing](/blog/ai-changing-code-review-testing).

**Concurrency.** **Races**, **deadlocks**, **thread** safety—AI **rarely** **models** these **correctly**. **Generated** code may **lack** **synchronisation** or **use** **wrong** **primitives**. **Mitigation:** **Human** design and **review** for **concurrency**-sensitive paths; **tests** (e.g. **stress**, **concurrency** tests) where **appropriate**; **avoid** relying on AI for **locking** or **async** **coordination** without **verification**.

---

## Consistency and style failures in depth

**Cross-file drift.** **Naming** (e.g. async suffix, DTO vs Model), **error** handling (exceptions vs Result), **structure** (where logic lives) can **differ** when **many** files are **generated** or **edited** by AI. **Why:** No **global** view of **repo**; each **suggestion** is **local**. **Mitigation:** **Linters**, **formatters**, **documented** **style** guides; **human** review for **architectural** and **cross-file** **consistency**; **codebase-aware** tools where **possible**. See [What Developers Want From AI](/blog/what-developers-want-from-ai-assistants) (Consistency) and [Impact on Code Quality](/blog/impact-ai-tools-code-quality-maintainability).

---

## Failure mode summary table

| Failure mode | Typical symptom | Mitigation |
|--------------|-----------------|------------|
| **Architecture** | Wrong layer, broken dependencies, monolith suggested | Humans own architecture; prompt within bounds; review for layering |
| **Edge cases** | Null/empty/boundary bugs in production | Tests for edge cases; review with null/boundaries in mind |
| **Security** | Injection, hardcoded secrets, weak crypto | Security review; never trust AI for auth/secrets/injection |
| **Consistency** | Style and naming drift across files | Linters, formatters, style guide, human review |
| **Domain** | Wrong business rule, rounding, dates | Domain experts; tests that encode rules; AI for scaffolding only |

Use this table as a **quick** **checklist** when **reviewing** or **scoping** AI use—see [What to do: review, tests, ownership](#what-to-do-review-tests-ownership).

---

## By language and stack

**Strong training data (e.g. JavaScript/TypeScript, C#/.NET, Python, React).** AI **often** produces **plausible** and **consistent** code; **failure** modes (architecture, security, edge cases) **still** apply—**review** and **tests** remain **essential**. **Niche or legacy (e.g. COBOL, internal DSLs, old frameworks).** **Less** **training** data; AI may **suggest** **generic** or **wrong** patterns. **Use** AI for **explanation** and **scaffolding**; **human** **ownership** for **logic** and **integration**. **Polyglot repos.** **Context** can **mix** **languages** and **confuse** the model; **limit** AI to **single-language** or **bounded** **areas** where **possible**. See [What AI IDEs Get Right and Wrong](/blog/ai-ides-what-they-get-right-wrong) (By stack and language).

---

## When to involve security or domain experts

**Security.** **Involve** **security** or **app-sec** **review** for **auth**, **secrets**, **injection**-prone code, **PII** handling, and **compliance**-sensitive paths. **Do not** rely on AI **suggestions** or **AI** **review** tools as **sufficient** for **sign-off**. **Domain.** **Involve** **domain** experts when **business** rules, **eligibility**, **rounding**, **dates**, or **regulations** are **encoded** in code; **tests** that **encode** rules **catch** **misunderstandings**; AI for **scaffolding** only. See [Securing APIs](/blog/securing-apis-dotnet), [OWASP](/blog/owasp-api-security-top-10), and [Domain-Driven Design](/blog/domain-driven-design-basics).

---

## Key terms

- **Local optima:** AI **optimises** for the **narrow** context (e.g. one file, one prompt) and may **violate** **system-wide** **constraints** (architecture, consistency).
- **Edge case:** **Rare** inputs, **boundaries** (null, empty, zero, max), **concurrency** (races, deadlocks); AI **often** **misses** these.
- **Consistency drift:** **Style** or **patterns** **differ** across **files** when AI **generates** or **edits** without **global** **view** of the **repo**.

---

## Mitigation checklist: before, during, and after

**Before using AI.** **Document** **layers** and **patterns** (e.g. [Clean Architecture](/blog/clean-architecture-dotnet), [SOLID](/blog/solid-principles-in-practice)) so **review** has a **baseline**. **Define** **scope** (e.g. no AI for **auth**, **payment**, **PII**). **Enable** **linters** and **formatters** so **mechanical** **drift** is **caught**.

**During use.** **Review** all AI-generated code for **architecture**, **security**, **edge** cases, and **consistency**. **Expand** AI-scaffolded tests for **edge** cases and **business** rules. **Reject** or **refactor** output that **violates** **standards**. **Require** **explanation** for **opaque** or **critical** code.

**After merge.** **Measure** **defect rate** and **time to change**; **refactor** **hotspots** when **debt** appears. **Revisit** **norms** when **signals** **worsen**. See [Impact on Code Quality](/blog/impact-ai-tools-code-quality-maintainability) (Checklist, When to tighten standards).

---

## Related reading

- **[Trade-Offs of AI Code Generation](/blog/trade-offs-ai-code-generation)** — Speed vs understanding, debt, learning; when to use AI vs when not to.
- **[Impact of AI Tools on Code Quality and Maintainability](/blog/impact-ai-tools-code-quality-maintainability)** — How to keep quality high with review, standards, and metrics.
- **[What AI IDEs Get Right — and What They Get Wrong](/blog/ai-ides-what-they-get-right-wrong)** — Strengths and weaknesses of AI IDEs; overlaps with this article.
- **[How AI Is Changing Code Review and Testing](/blog/ai-changing-code-review-testing)** — Using AI in review and test gen while keeping humans in the loop.
- **[Current State of AI Coding Tools in 2026](/blog/current-state-ai-coding-tools-2026)** — Landscape and adoption; how tools fit into the pipeline.
- **[What Developers Actually Want From AI Assistants](/blog/what-developers-want-from-ai-assistants)** — Context, control, consistency, clarity; aligning tools with needs.

---

## Common issues and challenges

- **Trusting AI for "one big change":** Letting AI **design** or **refactor** across many files often produces **inconsistent** or **broken** results. **Break** work into **bounded** tasks; **review** each step—see [How Developers Are Integrating AI](/blog/developers-integrating-ai-daily-workflows).
- **Missing edge cases in generated tests:** AI tends to generate **happy path** tests; **nulls**, **boundaries**, and **concurrency** are often missed. **Add** and **maintain** edge-case tests yourself—see [Testing strategies](/blog/testing-strategies-unit-integration-e2e).
- **Security "suggestions" that are wrong:** AI can suggest **insecure** defaults (e.g. weak crypto, string concatenation for SQL). **Never** rely on AI alone for auth, secrets, or injection—see [OWASP](/blog/owasp-api-security-top-10) and [Securing APIs](/blog/securing-apis-dotnet).

---

## Best practices and pitfalls

**Do:**

- Use AI for **repetition** and **scaffolding**; **review** and **test** everything; **own** architecture and security.
- **Break** work into **bounded** tasks (one layer, one responsibility); use [Clean Architecture](/blog/clean-architecture-dotnet) and [SOLID](/blog/solid-principles-in-practice) so generated code stays in the right place.
- **Encode** business rules and edge cases in **tests**; use **domain** experts for nuance—see [Domain-Driven Design](/blog/domain-driven-design-basics).

**Do not:**

- Trust AI for **architecture**, **security**, or **business rules** without verification.
- Assume **more AI** means **better** code; see [Current State of AI Coding Tools](/blog/current-state-ai-coding-tools-2026) and [Trade-Offs](/blog/trade-offs-ai-code-generation).

---

## Quick reference: when to verify

| If AI generated… | Verify for |
|------------------|------------|
| **Architecture / new feature** | Layering, dependencies, patterns |
| **Any security path** | Auth, secrets, injection, [OWASP](/blog/owasp-api-security-top-10) |
| **Logic / algorithms** | Edge cases, null, boundaries, concurrency |
| **Many files** | Consistency, style, call sites |
| **Business rules** | Domain correctness, tests |

---

## Summary

- AI **still fails** at **architecture**, **edge cases**, **security**, **consistency**, and **domain** nuance.
- **Mitigate** with **review**, **tests**, **ownership**, and **clear** boundaries; use AI where it **helps**, verify where it **does not**.
- For more see [Trade-Offs](/blog/trade-offs-ai-code-generation), [Code Quality](/blog/impact-ai-tools-code-quality-maintainability), and [What AI IDEs Get Right and Wrong](/blog/ai-ides-what-they-get-right-wrong).

## Position & Rationale

The article states where current AI coding tools tend to fail (architecture, edge cases, security depth, consistency, domain nuance) based on how they work: they optimise locally, they don't have full system or threat-model context, and they can suggest outdated or wrong patterns. That's a constraint of the technology, not a verdict on whether to use it. The stance is: use AI where it helps; verify and own where it doesn't.

## Trade-Offs & Failure Modes

Relying on AI for areas where it typically fails (e.g. architecture, security) without human verification increases the chance of wrong or inconsistent outcomes. Adding more review and tests mitigates but doesn't remove the risk. Failure modes: assuming AI output is correct for security or business rules; using AI for cross-file or system-wide consistency without human oversight.

## What Most Guides Miss

Many guides list "what AI can do" and underplay "where it fails and why." The failure modes are predictable from how the tools work (local optimisation, no full context). Another gap: mitigation is review, tests, and clear boundaries—not "better prompts" alone. Boundaries (e.g. "AI only within this layer") need to be explicit.

## Decision Framework

- **If the task is local, repetitive, or well-scoped** → AI often helps; still review the result.
- **If the task is architecture, security, or cross-cutting consistency** → Don't rely on AI alone; humans must own and verify.
- **For edge cases and business rules** → Assume AI can miss them; tests and review are the safety net.
- **For any use** → Define where AI is allowed and where human-only applies.

## Key Takeaways

- AI tends to fail at architecture, edge cases, security depth, consistency, and domain nuance; the causes are structural (local optimisation, lack of full context).
- Mitigate with review, tests, ownership, and clear boundaries; use AI where it helps, verify where it doesn't.
- Don't assume AI output is correct for security or business-critical code without human verification.

## When I Would Use This Again — and When I Wouldn't

Use this framing when a team wants a clear picture of where AI coding tools fall short so they can set boundaries and review accordingly. Don't use it to argue that AI is useless; the point is to match use to what the tools can and can't reliably do.

---

## Frequently Asked Questions

### Where does AI fail most in software development?

Most often: **architecture** (local optima), **edge cases** (rare inputs, boundaries), **security** (wrong or outdated advice), **consistency** (style drift), and **domain** (business rules). See the article sections above.

### Can AI do architecture and design?

Not reliably. AI tends to optimise **locally** and can break [Clean Architecture](/blog/clean-architecture-dotnet) or team patterns. **Humans** should own architecture; use AI for **implementation within** agreed boundaries.

### Is AI-generated code secure?

**Not by default.** AI can suggest insecure patterns. Never rely on AI alone for auth, secrets, or injection. Use security review and [Securing APIs](/blog/securing-apis-dotnet), [OWASP](/blog/owasp-api-security-top-10).

### How do I catch AI mistakes in edge cases?

[Unit and integration tests](/blog/testing-strategies-unit-integration-e2e), **review** with edge cases in mind, and **never** assume AI output is correct without verification. See [How AI Is Changing Code Review and Testing](/blog/ai-changing-code-review-testing).

### Why does AI produce inconsistent code across files?

AI has no **global** view of your codebase style; it can **drift**. Use **linters**, **formatters**, **style guides**, and **human review** to keep consistency.

### What are the trade-offs of relying on AI for code?

See [The Trade-Offs of Relying on AI for Code Generation](/blog/trade-offs-ai-code-generation): speed vs understanding, debt risk, and learning impact.

### Can AI help with legacy code?

Yes for **explanation** and **scaffolding** (e.g. tests, wrappers). **Risks** are higher: wrong assumptions, **breaking** callers. Use **small** steps and **review**—see [Developers integrating AI](/blog/developers-integrating-ai-daily-workflows).

### Why does AI suggest insecure code sometimes?

Models are trained on **public** code, which often contains **insecure** patterns. They do not "know" your **threat model**. Always **review** security-sensitive paths; use [Securing APIs](/blog/securing-apis-dotnet) and [OWASP](/blog/owasp-api-security-top-10).

### How do I know when to trust AI output?

**Trust** for **repetitive**, **pattern-based** tasks in **bounded** scope (one file, one layer). **Verify** for **architecture**, **security**, **edge cases**, and **business rules**. See [What to do: review, tests, ownership](#what-to-do-review-tests-ownership) above.

### Does AI fail less in some languages or stacks?

**Strong** training data (e.g. **JavaScript/TypeScript**, **C#/.NET**, **Python**) often yields **more** **plausible** code; **failure** **modes** (architecture, security, edge cases, consistency) **still** apply. **Niche** or **legacy** stacks **fail** **more** (wrong or **outdated** patterns)—use AI for **explanation** and **scaffolding**; **human** **ownership** for **logic**. See [By language and stack](#by-language-and-stack).

### When should we involve security or domain experts?

**Security:** For **auth**, **secrets**, **injection**, **PII**, **compliance**—**always** **human** **security** review; **never** AI-only. **Domain:** For **business** rules, **eligibility**, **rounding**, **dates**—**domain** experts and **tests** that **encode** rules; AI for **scaffolding** only. See [When to involve security or domain experts](#when-to-involve-security-or-domain-experts).
`,
  faqs: [
    { question: "Where does AI fail most in software development?", answer: "Architecture (local optima), edge cases (rare inputs, boundaries), security (wrong or outdated advice), consistency (style drift), and domain (business rules)." },
    { question: "Can AI do architecture and design?", answer: "Not reliably. Humans should own architecture; use AI for implementation within agreed boundaries. See Clean Architecture and the article." },
    { question: "Is AI-generated code secure?", answer: "Not by default. Use security review and Securing APIs, OWASP. Never rely on AI alone for auth, secrets, or injection." },
    { question: "How do I catch AI mistakes in edge cases?", answer: "Unit and integration tests, review with edge cases in mind, never assume AI output is correct. See testing strategies and How AI Is Changing Code Review and Testing." },
    { question: "Why does AI produce inconsistent code across files?", answer: "AI has no global view of your style; use linters, formatters, style guides, and human review." },
    { question: "What are the trade-offs of relying on AI for code?", answer: "See The Trade-Offs of Relying on AI for Code Generation: speed vs understanding, debt risk, learning impact." },
    { question: "Can AI help with legacy code?", answer: "Yes for explanation and scaffolding. Use small steps and review; risks are higher for wrong assumptions and breaking callers." },
    { question: "Why does AI suggest insecure code sometimes?", answer: "Models are trained on public code with insecure patterns; they do not know your threat model. Always review security-sensitive paths." },
    { question: "How do I know when to trust AI output?", answer: "Trust for repetitive, bounded tasks; verify for architecture, security, edge cases, and business rules." }
  ]
}
